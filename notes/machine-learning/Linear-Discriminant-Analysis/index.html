<!doctype html><html lang=en><head><meta charset=utf-8><meta name=description content="Idea:
 We have two classes and we assume that the features of each class form a cluster then we model each cluster as a gaussian distribution, which means we have a generative model i."><meta property="og:title" content="Linear Discriminant Analysis"><meta property="og:description" content="Idea:
 We have two classes and we assume that the features of each class form a cluster then we model each cluster as a gaussian distribution, which means we have a generative model i."><meta property="og:type" content="website"><meta property="og:image" content="https://notes.cakir-ufuk.de/icon.png"><meta property="og:url" content="https://notes.cakir-ufuk.de/notes/machine-learning/Linear-Discriminant-Analysis/"><meta property="og:width" content="200"><meta property="og:height" content="200"><meta name=twitter:card content="summary"><meta name=twitter:title content="Linear Discriminant Analysis"><meta name=twitter:description content="Idea:
 We have two classes and we assume that the features of each class form a cluster then we model each cluster as a gaussian distribution, which means we have a generative model i."><meta name=twitter:image content="https://notes.cakir-ufuk.de/icon.png"><title>Linear Discriminant Analysis</title><meta name=viewport content="width=device-width,initial-scale=1"><link rel="shortcut icon" type=image/png href=https://notes.cakir-ufuk.de//icon.png><link href=https://notes.cakir-ufuk.de/styles.80333fa2099c0bee674efa435fde378c.min.css rel=stylesheet><link href=https://notes.cakir-ufuk.de/styles/_light_syntax.86a48a52faebeaaf42158b72922b1c90.min.css rel=stylesheet id=theme-link><script src=https://notes.cakir-ufuk.de/js/darkmode.51c9298f1cc5576417c16208fea2435d.min.js></script>
<script src=https://notes.cakir-ufuk.de/js/util.00639692264b21bc3ee219733d38a8be.min.js></script>
<link rel=preload href=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css as=style onload='this.onload=null,this.rel="stylesheet"' integrity=sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js integrity=sha384-z1fJDqw8ZApjGO3/unPWUPsIymfsJmyrDVWC8Tv/a1HeOtGmkwNd/7xUS0Xcnvsx crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/auto-render.min.js integrity=sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.2/dist/contrib/copy-tex.min.js integrity=sha384-ww/583aHhxWkz5DEVn6OKtNiIaLi2iBRNZXfJRiY1Ai7tnJ9UXpEsyvOITVpTl4A crossorigin=anonymous></script>
<script src=https://cdn.jsdelivr.net/npm/@floating-ui/core@1.2.1></script>
<script src=https://cdn.jsdelivr.net/npm/@floating-ui/dom@1.2.1></script>
<script defer src=https://notes.cakir-ufuk.de/js/popover.aa9bc99c7c38d3ae9538f218f1416adb.min.js></script>
<script defer src=https://notes.cakir-ufuk.de/js/code-title.ce4a43f09239a9efb48fee342e8ef2df.min.js></script>
<script defer src=https://notes.cakir-ufuk.de/js/clipboard.2913da76d3cb21c5deaa4bae7da38c9f.min.js></script>
<script defer src=https://notes.cakir-ufuk.de/js/callouts.7723cac461d613d118ee8bb8216b9838.min.js></script>
<script>const SEARCH_ENABLED=!1,LATEX_ENABLED=!0,PRODUCTION=!0,BASE_URL="https://notes.cakir-ufuk.de/",fetchData=Promise.all([fetch("https://notes.cakir-ufuk.de/indices/linkIndex.bbe8389388affe768131a355cd37eb94.min.json").then(e=>e.json()).then(e=>({index:e.index,links:e.links})),fetch("https://notes.cakir-ufuk.de/indices/contentIndex.92c4c6dc2d6431fc9f3c5f8219b7233c.min.json").then(e=>e.json())]).then(([{index:e,links:t},n])=>({index:e,links:t,content:n})),render=()=>{const e=new URL(BASE_URL),t=e.pathname,n=window.location.pathname,s=t==n;addCopyButtons(),addTitleToCodeBlocks(),addCollapsibleCallouts(),initPopover("https://notes.cakir-ufuk.de",!0);const o=document.getElementById("footer");if(o){const e=document.getElementById("graph-container");if(!e)return requestAnimationFrame(render);e.textContent="";const t=s&&!0;drawGraph("https://notes.cakir-ufuk.de",t,[{"/moc":"#4388cc"}],t?{centerForce:1,depth:-1,enableDrag:!0,enableLegend:!1,enableZoom:!0,fontSize:.5,linkDistance:1,opacityScale:3,repelForce:1,scale:1.4}:{centerForce:1,depth:1,enableDrag:!0,enableLegend:!0,enableZoom:!0,fontSize:.6,linkDistance:1,opacityScale:3,repelForce:2,scale:1.2})}var i=document.getElementsByClassName("mermaid");i.length>0&&import("https://unpkg.com/mermaid@9/dist/mermaid.esm.min.mjs").then(e=>{e.default.init()});function a(n){const e=n.target,t=e.className.split(" "),s=t.includes("broken"),o=t.includes("internal-link");plausible("Link Click",{props:{href:e.href,broken:s,internal:o,graph:!1}})}const r=document.querySelectorAll("a");for(link of r)link.className.includes("root-title")&&link.addEventListener("click",a,{once:!0})},init=(e=document)=>{addCopyButtons(),addTitleToCodeBlocks(),renderMathInElement(e.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1}],macros:{'â€™':"'"},throwOnError:!1})}</script><script type=module>
    import { attachSPARouting } from "https:\/\/notes.cakir-ufuk.de\/js\/router.d6fe6bd821db9ea97f9aeefae814d8e7.min.js"
    attachSPARouting(init, render)
  </script><script defer data-domain=notes.cakir-ufuk.de src=https://plausible.io/js/script.js></script>
<script>window.plausible=window.plausible||function(){(window.plausible.q=window.plausible.q||[]).push(arguments)}</script></head><body><div id=search-container><div id=search-space><input autocomplete=off id=search-bar name=search type=text aria-label=Search placeholder="Search for something..."><div id=results-container></div></div></div><script src=https://cdn.jsdelivr.net/npm/flexsearch@0.7.21/dist/flexsearch.bundle.js integrity="sha256-i3A0NZGkhsKjVMzFxv3ksk0DZh3aXqu0l49Bbh0MdjE=" crossorigin=anonymous defer></script>
<script defer src=https://notes.cakir-ufuk.de/js/full-text-search.e6e2e0c213187ca0c703d6e2c7a77fcd.min.js></script><div class=singlePage><header><h1 id=page-title><a class=root-title href=https://notes.cakir-ufuk.de/>ðŸª´ Notes</a></h1><div class=spacer></div><div id=search-icon><p>Search</p><svg tabindex="0" aria-labelledby="title desc" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 19.9 19.7"><title id="title">Search Icon</title><desc id="desc">Icon to open search</desc><g class="search-path" fill="none"><path stroke-linecap="square" d="M18.5 18.3l-5.4-5.4"/><circle cx="8" cy="8" r="7"/></g></svg></div><div class=darkmode><input class=toggle id=darkmode-toggle type=checkbox tabindex=-1>
<label id=toggle-label-light for=darkmode-toggle tabindex=-1><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" id="dayIcon" viewBox="0 0 35 35" style="enable-background:new 0 0 35 35"><title>Light Mode</title><path d="M6 17.5C6 16.672 5.328 16 4.5 16h-3C.672 16 0 16.672.0 17.5S.672 19 1.5 19h3C5.328 19 6 18.328 6 17.5zM7.5 26c-.414.0-.789.168-1.061.439l-2 2C4.168 28.711 4 29.086 4 29.5 4 30.328 4.671 31 5.5 31c.414.0.789-.168 1.06-.44l2-2C8.832 28.289 9 27.914 9 27.5 9 26.672 8.329 26 7.5 26zm10-20C18.329 6 19 5.328 19 4.5v-3C19 .672 18.329.0 17.5.0S16 .672 16 1.5v3C16 5.328 16.671 6 17.5 6zm10 3c.414.0.789-.168 1.06-.439l2-2C30.832 6.289 31 5.914 31 5.5 31 4.672 30.329 4 29.5 4c-.414.0-.789.168-1.061.44l-2 2C26.168 6.711 26 7.086 26 7.5 26 8.328 26.671 9 27.5 9zM6.439 8.561C6.711 8.832 7.086 9 7.5 9 8.328 9 9 8.328 9 7.5c0-.414-.168-.789-.439-1.061l-2-2C6.289 4.168 5.914 4 5.5 4 4.672 4 4 4.672 4 5.5c0 .414.168.789.439 1.06l2 2.001zM33.5 16h-3c-.828.0-1.5.672-1.5 1.5s.672 1.5 1.5 1.5h3c.828.0 1.5-.672 1.5-1.5S34.328 16 33.5 16zM28.561 26.439C28.289 26.168 27.914 26 27.5 26c-.828.0-1.5.672-1.5 1.5.0.414.168.789.439 1.06l2 2C28.711 30.832 29.086 31 29.5 31c.828.0 1.5-.672 1.5-1.5.0-.414-.168-.789-.439-1.061l-2-2zM17.5 29c-.829.0-1.5.672-1.5 1.5v3c0 .828.671 1.5 1.5 1.5s1.5-.672 1.5-1.5v-3C19 29.672 18.329 29 17.5 29zm0-22C11.71 7 7 11.71 7 17.5S11.71 28 17.5 28 28 23.29 28 17.5 23.29 7 17.5 7zm0 18c-4.136.0-7.5-3.364-7.5-7.5s3.364-7.5 7.5-7.5 7.5 3.364 7.5 7.5S21.636 25 17.5 25z"/></svg></label><label id=toggle-label-dark for=darkmode-toggle tabindex=-1><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" id="nightIcon" viewBox="0 0 100 100" style="enable-background='new 0 0 100 100'"><title>Dark Mode</title><path d="M96.76 66.458c-.853-.852-2.15-1.064-3.23-.534-6.063 2.991-12.858 4.571-19.655 4.571C62.022 70.495 50.88 65.88 42.5 57.5 29.043 44.043 25.658 23.536 34.076 6.47c.532-1.08.318-2.379-.534-3.23-.851-.852-2.15-1.064-3.23-.534-4.918 2.427-9.375 5.619-13.246 9.491-9.447 9.447-14.65 22.008-14.65 35.369.0 13.36 5.203 25.921 14.65 35.368s22.008 14.65 35.368 14.65c13.361.0 25.921-5.203 35.369-14.65 3.872-3.871 7.064-8.328 9.491-13.246C97.826 68.608 97.611 67.309 96.76 66.458z"/></svg></label></div></header><article><h1>Linear Discriminant Analysis</h1><p class=meta>Last updated
May 12, 2023
<a href=https://github.com/ufuk-cakir/notes/notes/machine-learning/Linear%20Discriminant%20Analysis.md rel=noopener>Edit Source</a></p><ul class=tags></ul><aside class=mainTOC><details><summary>Table of Contents</summary><nav id=TableOfContents></nav></details></aside><p>Idea:</p><ul><li>We have two classes and we assume that the features of each class form a cluster</li><li>then we model each cluster as a gaussian distribution, which means we have a generative model i.e the RHS of Bayes</li><li>Calculate the Posterior from the bayes formula</li></ul><p>Variant 1: Quadratic Discriminant Analysis (QDA)</p><ul><li>both clusters can have different shapes</li></ul><p>Variant 2: LDA</p><ul><li>both clusters have the same shape</li><li>-> LHS of Bayes reduces to a linear decision rule i.e $\hat y = sign(x\beta + b)$</li><li>all the nonlinear terms cancel out</li></ul><p>First of all: How do get to a gaussian?</p><p>Clusters are assumed to be elliptic (circles are unrealistic).</p><ul><li>Start from unit circle</li><li>Step 1: axis aligned scaling. Scale the unit circle with $\lambda=\left(\begin{array}{ll}r_1 & 0 \\ 0 & r_2\end{array}\right)$</li><li>Step 2: Rotation around origin</li><li>Step 3: Translate to the mean</li></ul><p>$$z = \lambda^{-1} Q^{-1}(x-\mu)$$
$$z = \lambda^{-1} Q^T(x-\mu)$$
<a href=/notes/machine-learning/Gaussian-Distribution-in-1-D rel=noopener class=internal-link data-src=/notes/machine-learning/Gaussian-Distribution-in-1-D>Gaussian Distribution in 1-D</a></p><p>Clusters are unit circles around the origin:
$$N\left(z \mid \mu=0, \Sigma=1\right)=\frac{1}{\sqrt{(2 \pi)^d}} \exp \left(-\frac{1}{2} z z^{\top}\right)$$
where d is the dimension</p><p><a href=/notes/machine-learning/Multi-variate-Gaussian rel=noopener class=internal-link data-src=/notes/machine-learning/Multi-variate-Gaussian>Multi variate Gaussian</a></p><p>We can write
$$\Sigma = Q\lambda^2Q^T$$
which is the Eigendecomposition of $\Sigma$, with the eigenvalues $\lambda$ and the Eigenvectors $Q$</p><hr><p>Now lets fit a gaussian to a cluster. We with only one cluster</p><p>$$N\left(x_i \mid \mu_1, \Sigma_1\right)=\frac{\exp \left(-\frac{1}{2}(\mathbf{x_i}-\boldsymbol{\mu_1})^{\mathrm{T}} \boldsymbol{\Sigma_1}^{-1}(\mathbf{x_i}-\boldsymbol{\mu_1})\right)}{\sqrt{\det{2\pi\Sigma_1}}}$$</p><p>Learning Problem: Find the mean and covariance of class 1 $\mu_1,\Sigma_1$</p><p>Fundamental Principle: Choose $\mu_1,\Sigma_1$ such that the Training set will be a typical outcome of the resulting model. You want to choose the model such that If you draw data from the model it should look very similar to the trainin set. This is called the
<strong>Maximum Likelihood Principle</strong>: The best model maximises the likelihood of the Trainin set</p><ul><li>Second Principle: i.i.d assumption: Training instances are drawn <em>independently</em> from the trainin set, and they are <em>identically distributed</em>
Why is this important? We can simplify the TS distribution:
Independent means that the joint distribution is the product of the individual distributions. Identically distributed means that all the instances come from the same prob-distribution, so I dont need a $p_i$
$$p(TS) = p(x_1, \dots, x_n)= \prod p(x_i)$$</li></ul><p>Maximum Likelihood:
$$ \hat \mu, \hat\Sigma = \arg \max_{\mu,\Sigma} p(TS)$$</p><p>Mathematically simpler: minimize the negative Logarithm of p(TS)</p><p>$$ \arg \max_{\mu,\Sigma} p(TS)\leftrightarrow \arg \min_{\mu,\Sigma} (-\log p(TS))$$</p><p>Because if you apply a monotic function to an optimzation problem the arg max will not change. Taking a minus takes a maximum into a minimum</p><p>$$ \hat \mu, \hat\Sigma = -\arg \min_{\mu,\Sigma} \sum_i^N\left( \log \left(\frac{1}{\sqrt{\det(2\pi\Sigma)}} \right)- \frac{1}{2}(x_{i}- \mu)\Sigma^{-1}(x_{i}- \mu)^{T}
\right)$$</p><p>$$ \hat \mu, \hat\Sigma = \arg \min_{\mu,\Sigma} \sum_i^N\left( \frac{1}{2} \log \left(\sqrt{\det(2\pi\Sigma)} \right)+ \frac{1}{2}(x_{i}- \mu)\Sigma^{-1}(x_{i}- \mu)^{T}
\right)$$
We can get rid of the scale facter 1/2 since it just scales and has no influence on the arg min. This is now our LOSS(TS)</p><p>General rule:</p><p>$$\log \det(2 \pi \Sigma) = \log ((2\pi)^{D}\det(\Sigma)) = \log(2\pi)^{D}+ \log \det \Sigma$$</p><p>Derivative Rule from Linear Algebra:</p><p>$$\frac{\partial vAv^{T}}{\partial v}= 2Av^T$$
$$\frac{\partial LOSS(TS)}{\partial \mu} = \sum_{i} \Sigma^{-1}(x_{i}-\mu)^T=0 $$
Multiply with $\Sigma$ from the left:
$$\sum\limits_{i}(x_{i}-\mu)^T=0$$
$$\sum\limits_{i}x_{i}=\sum\limits_{i}\mu^{T}= N \mu^T$$
$$\mu = \frac{1}{N} \sum\limits_{i}x_{i}$$
The optimal $\mu$ is the average. This is a mathematical proof that the average maximizes the likelihood of the data.</p><p>$$\frac{\partial LOSS(TS)}{\partial \Sigma}$$ is inconvient.</p><p>The first term is rather easy</p><p>$$\frac{\partial \log\det\Sigma}{\partial \Sigma} = -(\Sigma^{T)^{-1}}= -\Sigma^{-1} $$
since $\Sigma$ is symmetric.</p><p>Introduce precision matrix
$$K = \Sigma^{-1}$$
Calculate:
$$\frac{\partial}{\partial K} \sum\limits_i^{N}(x_i-\mu)K(x_{i}-\mu)^{T} = \sum (x_{i}-\mu)^{T} (x_{i}-\mu)$$
Where the last term is called the scalar matrix.</p><p>Put everything together:
(kein bock mehr zu schreiben) Lecuture session 5,2023-05-02 Minute 1:01:11</p><a href=#why-is-lda-a-linear-classifier><h1 id=why-is-lda-a-linear-classifier><span class=hanchor arialabel=Anchor># </span>Why is LDA a linear classifier?</h1></a><p>Priors:</p><p>$$
p(y=1)=\frac{N_1}{N} \quad p(y=-1)=\frac{N_{-1}}{N}
$$
assume for simplicity that these two are the same .i.e 1/2</p><p>The Gaussian for class 1 is given by
$$
p(x | y=1)=\frac{1}{\sqrt{\operatorname{det}(2 \pi \Sigma)}} \exp \left(-\frac{1}{2}\left(x- \mu_1\right) \Sigma^{-1}(x-\mu_1)^{\top}\right)
$$
The decision rule is then given by the argmax of the posterior probability</p><p>$$
\hat{y}_i=\operatorname{argmax}_k p\left(Y=k \mid x_i\right)
$$
inserting bayes rule gives</p><p>$$
\hat{y}<em>i=\operatorname{argmax}</em>{k}\frac{p\left(x_i \mid y=k\right) p\left(y=k\right)}{\sum_{k^{\prime}} p\left(x_i \mid y=k^{\prime}\right) p\left(y=k^{\prime}\right)}=\begin{cases}1 & \text { if } p\left(y=1 \mid x_i\right)>\frac{1}{2} \\ -1 & \text { if } p\left(y=1| x_i\right)&lt;\frac{1}{2} \\ & \Leftrightarrow p\left(y =-1 \mid x_i\right)>\frac{1}{2}\end{cases}
$$</p><p>Calculate posterior analytically:
$$
p(y=1 \mid x)=\frac{p(x \mid y=1) p(y=1)}{p(x \mid y=1) p(y=1)+p(x \mid y=-1) \mid p(y=-1)}
$$
$$=\frac{p(x \mid y=1)}{p(x \mid y=1)+p \mid(x \mid y=-1)}$$
$$\begin{equation}
\begin{aligned}
& =\frac{1}{1+\frac{p(x \mid y=-1)}{p(x \mid y=1)}} \\ & \frac{p(x \mid y=-1)}{p(x \mid y=1)}=\exp \left(-x \Sigma^{-1}\left(\mu_1^{\top}-\mu_{-1}^{\top}\right)-\frac{1}{2}\left(\mu_{-1} \Sigma^{-1} \mu_{-1}^{\top}-\mu_1 \Sigma^{-1} \mu_1^T\right)\right)=\exp (-(x \beta+b))
\end{aligned}
\end{equation}$$</p><p>So we get:
$$
p(y=1\mid x)=\frac{1}{1+\operatorname{exp}(-(x \beta+b))}
$$
which is called the (logistic) sigmoid function
<a class="internal-link broken">Unbenannt.png</a></p><p>This satisfies:
$$
\sigma(-t)=1-\sigma(t)
$$
$$
\frac{d}{dt}\sigma(t)=\sigma(t) \cdot \sigma(-t)
$$</p><p>So the posterior is simply
$$
p(y=1 \mid x)=\sigma(x \beta+b)
$$
$$
p(y=-1 \mid x)=\sigma(-(x \beta+b))
$$
where
$$
\begin{aligned}
& \beta=\Sigma^{-1}\left(\mu_1^{\top}-\mu_{-1}^{\top}\right) \\ & \left.b=\frac{1}{2}\left(\mu_{-1}\Sigma^{-1} \mu_{-1}^{\top}-\mu_1 \Sigma^{-1} \mu_{1}^{\top}\right)\right)
\end{aligned}
$$
So the decision rule is
$$
\hat{y}=\underset{k}{\operatorname{argmax}} p(y=k \mid x)
$$
$$
\Leftrightarrow\left{\begin{array}{lll}
1 & \text { if } & \sigma(x \beta+b)>\frac{1}{2} \\ -1 & \text { if } & \sigma(-(x \beta+b))>\frac{1}{2}
\end{array}\right.
$$
$$
\Leftrightarrow\left{\begin{array}{lll}
1 & \text { if } & x \beta+b>0 \\ -1 & \text { if } & x \beta+b&lt;0
\end{array}\right.
$$
$$
\hat{y}=\operatorname{sign}(x\beta+b)
$$</p><p>Which is linear!!!</p></article><hr><div class=page-end id=footer><div class=backlinks-container><h3>Backlinks</h3><ul class=backlinks><li>No backlinks found</li></ul></div><div><script src=https://cdn.jsdelivr.net/npm/d3@6.7.0/dist/d3.min.js integrity="sha256-+7jaYCp29O1JusNWHaYtgUn6EhuP0VaFuswhNV06MyI=" crossorigin=anonymous></script><h3>Interactive Graph</h3><div id=graph-container></div><style>:root{--g-node:var(--secondary);--g-node-active:var(--primary);--g-node-inactive:var(--visited);--g-link:var(--outlinegray);--g-link-active:#5a7282}</style><script src=https://notes.cakir-ufuk.de/js/graph.6579af7b10c818dbd2ca038702db0224.js></script></div></div><div id=contact_buttons><footer><p>Made by Ufuk Cakir using <a href=https://github.com/jackyzha0/quartz>Quartz</a>, Â© 2023</p><ul><li><a href=https://notes.cakir-ufuk.de/>Home</a></li><li><a href=https://github.com/ufuk-cakir/>GitHub</a></li></ul></footer></div></div></body></html>