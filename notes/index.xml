<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Notes on</title><link>https://notes.cakir-ufuk.de/notes/</link><description>Recent content in Notes on</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><atom:link href="https://notes.cakir-ufuk.de/notes/index.xml" rel="self" type="application/rss+xml"/><item><title/><link>https://notes.cakir-ufuk.de/notes/machine-learning/covariance-matrix/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://notes.cakir-ufuk.de/notes/machine-learning/covariance-matrix/</guid><description/></item><item><title/><link>https://notes.cakir-ufuk.de/notes/machine-learning/cross-entropy-loss/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://notes.cakir-ufuk.de/notes/machine-learning/cross-entropy-loss/</guid><description>?
Solve by gradient decent since it has no analytic solution</description></item><item><title>Alternatives for learnine beta and b</title><link>https://notes.cakir-ufuk.de/notes/machine-learning/learning-beta-and-b/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://notes.cakir-ufuk.de/notes/machine-learning/learning-beta-and-b/</guid><description>Alternatives for learning $\beta$ and b fit mean and covariance of clusters least- squares regression: $$ \operatorname{loss}\left(Y_i^\ast, \hat{Y}_i\right)=\left(Y_i^\ast-(x \beta+b)\right)^2 $$ Fishers original idea: define 1D scores: $$z_{i}= x_{i}\beta$$ and choose beta such that a threshold classifier on z_i has minimum error define Projection of means $$ \tilde{\mu}1=\mu{1} \beta \quad \tilde{\mu}{-1}=\mu{-1} \beta $$ The intuition tells us that $\tilde{\mu}1$ and $\tilde{\mu}{-1}$ should be as far apart as possible, so one could suggest that$$ \beta=\arg \max _\mu\left(\tilde{\mu}1-\tilde{\mu}{-1}\right)^2 $$ But this doesnt work!</description></item><item><title>Backpropagation</title><link>https://notes.cakir-ufuk.de/notes/machine-learning/backpropagation/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://notes.cakir-ufuk.de/notes/machine-learning/backpropagation/</guid><description>Training Neural Networks by Backpropagation Backpropagation is simply a fancier name for the chain rule of calculus.
We want to train $B_1,\dots,B_L$ (The Matrix of weight vectors, which are column vectors) by gradient decent, so we need the derivative of</description></item><item><title>Conjugate Prior</title><link>https://notes.cakir-ufuk.de/notes/lectures/computational-statistics/Conjugate-Prior/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://notes.cakir-ufuk.de/notes/lectures/computational-statistics/Conjugate-Prior/</guid><description/></item><item><title>Cross Validation</title><link>https://notes.cakir-ufuk.de/notes/machine-learning/cross-validation/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://notes.cakir-ufuk.de/notes/machine-learning/cross-validation/</guid><description>How to find good parameter? The Support Vector Machine has a hyper parameter: relative Weight of the data and regularization term.</description></item><item><title>diffeomorphisms</title><link>https://notes.cakir-ufuk.de/notes/diffeomorphisms/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://notes.cakir-ufuk.de/notes/diffeomorphisms/</guid><description>Definition [!info] Given two manifolds $M$ and $N$, a differentiable map $f: M \rightarrow N$ is called a diffeomorphism if it is a bijection and its inverse $f^{-1}: N \rightarrow M$ is differentiable as well.</description></item><item><title>Exponential Family</title><link>https://notes.cakir-ufuk.de/notes/lectures/computational-statistics/Exponential-Family/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://notes.cakir-ufuk.de/notes/lectures/computational-statistics/Exponential-Family/</guid><description/></item><item><title>Functional Programming</title><link>https://notes.cakir-ufuk.de/notes/python/Functional-Programming/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://notes.cakir-ufuk.de/notes/python/Functional-Programming/</guid><description>The main idea of functional programming is to organize the code around the use of functions. Each function should perform only one clearly defined task and needs to be pure.</description></item><item><title>Gaussian Distribution in 1-D</title><link>https://notes.cakir-ufuk.de/notes/machine-learning/Gaussian-Distribution-in-1-D/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://notes.cakir-ufuk.de/notes/machine-learning/Gaussian-Distribution-in-1-D/</guid><description>$$N\left(x \mid \mu, \sigma^2\right)=\frac{1}{\sqrt{2 \pi \sigma^2}} \exp \left(-\frac{1}{2}\left(\frac{(x-\mu)^2}{\sigma^2}\right)\right.$$
Standard Normal Distribution: $\mu = 0, \sigma^2 = 1$</description></item><item><title>Lasso Regression</title><link>https://notes.cakir-ufuk.de/notes/lectures/computational-statistics/Lasso-Regression/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://notes.cakir-ufuk.de/notes/lectures/computational-statistics/Lasso-Regression/</guid><description/></item><item><title>Linear Discriminant Analysis</title><link>https://notes.cakir-ufuk.de/notes/machine-learning/lda/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://notes.cakir-ufuk.de/notes/machine-learning/lda/</guid><description>Idea:
We have two classes and we assume that the features of each class form a cluster then we model each cluster as a gaussian distribution, which means we have a generative model i.</description></item><item><title>Logisitic Regression</title><link>https://notes.cakir-ufuk.de/notes/machine-learning/logistic-regression/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://notes.cakir-ufuk.de/notes/machine-learning/logistic-regression/</guid><description>Use the same posterior as [[content/notes/machine-learning/lda]], but train LHS of Bayes rule, which gives a different solution.
i.i.d assumption: all labels are drawn independently from same posterior</description></item><item><title>Machine Learning Essentials</title><link>https://notes.cakir-ufuk.de/notes/machine-learning/machine-learning-essentials/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://notes.cakir-ufuk.de/notes/machine-learning/machine-learning-essentials/</guid><description>Here are my Lecture Notes of the Machine Learning Essentials Lecture by Ullrich KÃ¶the , SS 2023, Heidelberg.
02.05.2023: [[content/notes/machine-learning/cross validation]] [[content/notes/machine-learning/lda]] [[content/notes/machine-learning/learning beta and b]] 09.</description></item><item><title>Mode Collapse</title><link>https://notes.cakir-ufuk.de/notes/machine-learning/mode-collapse/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://notes.cakir-ufuk.de/notes/machine-learning/mode-collapse/</guid><description>Copied from: [https://machinelearning.wtf/terms/mode-collapse/]
Mode collapse, also known as catastrophic collapse or the Helvetica scenario, is a common problem when training generative adversarial networks (GANs).</description></item><item><title>Multi Class Classification</title><link>https://notes.cakir-ufuk.de/notes/machine-learning/multi-class-classification/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://notes.cakir-ufuk.de/notes/machine-learning/multi-class-classification/</guid><description>Two Possibilites:
reduce to a set of 2-class problems or: use a more powerful model One- against-rest classification For each class $k=1,.</description></item><item><title>Multi variate Gaussian</title><link>https://notes.cakir-ufuk.de/notes/machine-learning/Multi-variate-Gaussian/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://notes.cakir-ufuk.de/notes/machine-learning/Multi-variate-Gaussian/</guid><description/></item><item><title>Multivariate Gaussian</title><link>https://notes.cakir-ufuk.de/notes/lectures/computational-statistics/Multivariate-Gaussian/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://notes.cakir-ufuk.de/notes/lectures/computational-statistics/Multivariate-Gaussian/</guid><description>[!note] This note is not complete. If you wich the extend this note, read the information on the welcome page.</description></item><item><title>Navier-Stokes Equations</title><link>https://notes.cakir-ufuk.de/notes/physics/Navier-Stokes-Equations/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://notes.cakir-ufuk.de/notes/physics/Navier-Stokes-Equations/</guid><description>$$ \frac{\partial \vec{v}}{\partial t}+(\vec{v} \cdot \vec{\nabla}) \vec{v}=-\frac{1}{\rho} \vec{\nabla} p-2(\vec{\Omega} \times \vec{v})+\nu \Delta \vec{v}+\vec{g} $$
$\frac{\partial \vec{v}}{\partial t}$: This term represents the partial derivative of the velocity vector (\vec{v}) with respect to time (\partial t).</description></item><item><title>Neural Networks</title><link>https://notes.cakir-ufuk.de/notes/machine-learning/neural-networks/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://notes.cakir-ufuk.de/notes/machine-learning/neural-networks/</guid><description>Definition of a Neuron:
inputs $Z_{in}$ (row vector) computation: Linear function followed by non-linearity $$Z_{out}=\varphi(Z_{in}\cdot\beta+b)$$ which we call the activation of a Neuron.</description></item><item><title>Non Linear Classification</title><link>https://notes.cakir-ufuk.de/notes/machine-learning/Non-Linear-Classification/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://notes.cakir-ufuk.de/notes/machine-learning/Non-Linear-Classification/</guid><description>Used when Data is not Linear separable
Bad Practise: Use LR and hope for the best. Exception: Very sparse data, when its hard to justify non-linear fit Measure more features, increase $D$ or measure $x_i$ more accuratly.</description></item><item><title>Normalizing Flows</title><link>https://notes.cakir-ufuk.de/notes/machine-learning/Normalizing-Flows/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://notes.cakir-ufuk.de/notes/machine-learning/Normalizing-Flows/</guid><description>This is a summary of Papamakarios et al. 2021 https://arxiv.org/abs/1912.02762
Introduction to Normalizing Flows A normalizing flow simply transforms a simple densty multiple times to produce a richer and more complicated distribution.</description></item><item><title>Object Oriented Programming</title><link>https://notes.cakir-ufuk.de/notes/python/Object-Oriented-Programming/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://notes.cakir-ufuk.de/notes/python/Object-Oriented-Programming/</guid><description>Object Oriented Programming (OOP) is a [[notes/python/Programming Paradigm]]</description></item><item><title>Procedural Programming</title><link>https://notes.cakir-ufuk.de/notes/python/Procedural-Programming/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://notes.cakir-ufuk.de/notes/python/Procedural-Programming/</guid><description>Procedural Programming is a [[notes/python/Programming Paradigm|Programming Paradigm]]. The idea is to get things done in a sequence of steps i.e think of the code as a step-by-steo course of action that needs to be executed.</description></item><item><title>Programming Paradigm</title><link>https://notes.cakir-ufuk.de/notes/python/Programming-Paradigm/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://notes.cakir-ufuk.de/notes/python/Programming-Paradigm/</guid><description/></item><item><title>Regularization</title><link>https://notes.cakir-ufuk.de/notes/lectures/computational-statistics/Regularization/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://notes.cakir-ufuk.de/notes/lectures/computational-statistics/Regularization/</guid><description/></item><item><title>Ridge Regression</title><link>https://notes.cakir-ufuk.de/notes/lectures/computational-statistics/Ridge-Regression/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://notes.cakir-ufuk.de/notes/lectures/computational-statistics/Ridge-Regression/</guid><description/></item><item><title>Softmax Function</title><link>https://notes.cakir-ufuk.de/notes/machine-learning/Softmax-Function/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://notes.cakir-ufuk.de/notes/machine-learning/Softmax-Function/</guid><description>$$ p\left(\hat{Y}i=k \mid X_i\right)=\frac{\exp \left(s_k\right)}{\sum{\dot{k}^{\prime}=1}^c \exp \left(s_{k^{\prime}}\right)}=\operatorname{softmax}(s_i,..,s_c) $$</description></item><item><title>Softplus Function</title><link>https://notes.cakir-ufuk.de/notes/machine-learning/Softplus-Function/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://notes.cakir-ufuk.de/notes/machine-learning/Softplus-Function/</guid><description>$$ -\log \sigma(t)=-\log \frac{1}{1+\exp (-t)}=\log (1+\exp (-t)) $$ Is like a smoothed Version of ReLu</description></item><item><title>Summary Linear Classification</title><link>https://notes.cakir-ufuk.de/notes/machine-learning/Summary-Linear-Classification/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://notes.cakir-ufuk.de/notes/machine-learning/Summary-Linear-Classification/</guid><description>Summary of Linear Classification with $y \in{-1,1}$ : All methods have the same decision Rule
$$ \hat{y}_i=\operatorname{sign}\left(x_i \cdot \beta+b\right) $$</description></item><item><title>Variational Inference with Normalizing Flows Paper</title><link>https://notes.cakir-ufuk.de/notes/machine-learning/Variational-Inference-with-Normalizing-Flows-Paper/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://notes.cakir-ufuk.de/notes/machine-learning/Variational-Inference-with-Normalizing-Flows-Paper/</guid><description>https://arxiv.org/pdf/1505.05770v6.pdf
Fragen: What is variational Inference? Why is posterior approximation in variational inference a disadvantage inferential and variational methods difference Notes on Paper: variational methods have huge succes and ongoing advances, but have a number of disadvantges.</description></item><item><title>Youtube Tutorial Brubaker Normalizing Flows</title><link>https://notes.cakir-ufuk.de/notes/machine-learning/Youtube-Tutorial-Brubaker-Normalizing-Flows/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://notes.cakir-ufuk.de/notes/machine-learning/Youtube-Tutorial-Brubaker-Normalizing-Flows/</guid><description>https://www.youtube.com/watch?v=u3vVyFVU_lI&amp;ab_channel=MarcusBrubaker
What are Normalizing Flows? Probabilistic Generative Model built on invertible transformaitons They are Generally: Efficient to sample from $p_X(x)$ Efficient to evaulate $p_X(x)$ almost exactly highly (flexibly) expressive useful latent representation straightforward to train What are NF now mathematically?</description></item><item><title>ðŸ“š Lecture: Computational Statistics and Data Analysis</title><link>https://notes.cakir-ufuk.de/notes/lectures/computational-statistics/Computational-Statistics-and-Data-Analysis/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://notes.cakir-ufuk.de/notes/lectures/computational-statistics/Computational-Statistics-and-Data-Analysis/</guid><description>[!note] This note is not complete. If you wich the extend this note, read the information on the welcome page.</description></item></channel></rss>