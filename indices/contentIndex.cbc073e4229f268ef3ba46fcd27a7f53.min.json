{"/":{"title":"ü™¥ Notes","content":"\n\n# Welcome!\nOn these Pages you can find some lectures notes and other stuff!\n\n---\nCheck for example:\n- [[notes/machine-learning/Machine Learning Essentials]]: Lecture Notes\n\n\n\n\n\n","lastmodified":"2023-10-07T08:19:48.782592133Z","tags":[]},"/notes/diffeomorphisms":{"title":"diffeomorphisms","content":"\n# Definition\n\n\u003e [!info]\n\u003e Given two manifolds $M$ and $N$, a differentiable map $f: M \\rightarrow N$ is called a diffeomorphism if it is a bijection and its inverse $f^{-1}: N \\rightarrow M$ is differentiable as well. If these functions are $r$ times continuously differentiable, $f$ is called a $C^r$-diffeomorphism. ([Wikipedia](https://en.wikipedia.org/wiki/Diffeomorphism))\n","lastmodified":"2023-10-07T08:19:48.782592133Z","tags":[]},"/notes/machine-learning/Alternatives-for-learning-beta-and-b":{"title":"Alternatives for learnine beta and b","content":"\n## Alternatives for learning $\\beta$ and b\n1. fit mean and covariance of clusters\n2. least- squares regression: $$\n\\operatorname{loss}\\left(Y_i^*, \\hat{Y}_i\\right)=\\left(Y_i^*-(x \\beta+b)\\right)^2\n$$\n3. Fishers original idea: define 1D scores: \n4. $$z_{i}= x_{i}\\beta$$\n5. and choose beta such that a threshold classifier on z_i has minimum error\ndefine Projection of means \n$$\n\\tilde{\\mu}_1=\\mu_{1} \\beta \\quad \\tilde{\\mu}_{-1}=\\mu_{-1} \\beta\n$$\n\nThe intuition tells us that $\\tilde{\\mu}_1$ and $\\tilde{\\mu}_{-1}$ should be as far apart as possible, so one could suggest that$$\n\\beta=\\arg \\max _\\mu\\left(\\tilde{\\mu}_1-\\tilde{\\mu}_{-1}\\right)^2\n$$\nBut this doesnt work!\nSince we could scale $\\beta$ by any number $\\tau$ and get the same projection, but make the difference as big as we want.\n\nSolution: Scale by the variance!\n\n$$\n\\sigma_1^2=\\operatorname{Var}\\left(z_i \\mid Y_i^*=1\\right) \\quad \\sigma_{-1}^2=\\operatorname{Var}\\left(z_i \\mid Y_i^*=-1\\right)\n$$\n\nAnd then choose\n$$\n\\hat{\\beta}=\\operatorname{argmax}_p \\frac{\\left(\\tilde{\\mu}_1-\\tilde{\\mu}_{-1}\\right)^2}{\\tilde\\sigma_1^2+\\tilde{\\sigma}_{-1}^2}\n$$\nWhich gives the same solution as 1. and 2.\n\n\nBut is there a way we get a different solution?\n\nYES!\n\n![[Logisitic Regression]]","lastmodified":"2023-10-07T08:19:48.786592134Z","tags":[]},"/notes/machine-learning/Backpropagation":{"title":"Backpropagation","content":"\n\n# Training Neural Networks by Backpropagation\n\nBackpropagation is simply a fancier name for the chain rule of calculus.\n\n\nWe want to train $B_1,\\dots,B_L$  (The Matrix of weight vectors, which are column vectors) by gradient decent, so we need the derivative of\n\n$$\\frac{\\partial \\operatorname{loss}}{\\partial B_l}$$\nand then update by\n$$B_l^{(t)}=B_l^{(t-1)}-\\tau \\cdot \\frac{\\partial\\text{loss}}{\\partial B_l^{(t-1)}}$$\n\n1. Step:  Calculate derivative of the loss w.r.t Network output: $$\\frac{\\partial \\operatorname{loss}}{\\partial z_l}$$ The loss is application dependent: For example for **regression** we have $$\\text{loss}=\\frac{1}{2}\\left(z_L-y_i^\\ast\\right)^2\\qquad\\frac{\\partial \\operatorname{loss}}{\\partial z_L}=z_L-y_i^\\ast$$ Or for classication we have the **cross entropy loss**:  $$\\text{loss}=-\\sum_{k=1}^c \\mathbb{1}\\left[ k=y_i^{*}\\right]\\log \\frac{p(Y=k \\mid X)}{z_{Lk}}$$$$\\frac{\\partial \\operatorname{loss}}{\\partial z_{L k}}=\\left\\{\\begin{array}{c}-\\frac{1}{z_{Lk}} \\qquad\\text{if } k=y_i^\\ast \\\\ 0  \\qquad \\text { otherwise }\\end{array}\\right.$$\n2.  Step: Backpropagate through output activation $\\varphi_L(\\tilde z_L$). First define $$\\tilde{\\delta}_L=\\frac{\\partial L}{\\partial \\tilde z_L}$$ If you go thorugh the calculations you will arrive at $$\\tilde \\delta_L=\\frac{\\partial \\operatorname{loss}}{\\partial \\tilde z_{L k}}=\\left\\{\\begin{array}{c}z_{L_k} -1 \\qquad\\text{if } k=y_i^\\ast \\\\ z_{L_k}  \\qquad \\text { otherwise }\\end{array}\\right.$$ This makes sense in the following way: If for a k the true lable is 1, ideally we want the probability for the true label to be 1. But if $z_{L_k}-1$ is less than 1 we go to the opposite direction i.e we make it bigger. Similarly the probability for the wrong label should be zero so if $z_{L_k}$ is bigger than zero we go to the opposite direction and make it smaller.\n3. Step: For every $l = 1, \\dots, L$ let $\\tilde{\\delta}_L=\\frac{\\partial L}{\\partial \\tilde z_L}$. Recursion start $\\tilde\\delta_L$ on right $$\\tilde{\\delta}_{l-1}=\\frac{\\partial \\operatorname{loss}}{\\partial \\tilde{z}_{l-1}}=\\frac{\\partial \\operatorname{loss}}{\\partial \\tilde{z}_l} \\cdot \\frac{\\partial \\tilde z_l}{\\partial z_{l-1}} \\cdot\\frac{\\partial z_{l-1}}{\\partial \\tilde z_{l-1}}$$ So the Backpropagation step is basically \n\u003e [! note] $$\\tilde{\\delta_{l-1}}=\\frac{\\partial \\operatorname{loss}}{\\partial\\tilde{z}_{l-1}}=\\tilde{\\delta}_l \\cdot B_l^{\\top} \\cdot \\operatorname{diag}\\left(\\varphi^{\\prime}\\left(\\tilde{z}_{l-1}\\right)\\right)$$\n4. Step $$\\frac{\\partial {\\text {loss}}}{\\partial B_l}=\\mathcal{z}_{l-1}^{\\top} \\cdot \\tilde{\\delta}_l$$ This is an outer product since z and $\\tilde\\delta_L$ both are row vectors. We need to store $z_l$ during the forward propagation because we need it for backpropagation. So we need a GPU with large RAM.\n\n# Training Algorithm\n\n0. Init $B_e^{(0)}$ randomly\n1. for $t=1,..,T$ :\n\t1. forward pass: for $i$ in batch: $$\\begin{aligned}\nz_0 \u0026 =\\left[1, x_i\\right] \\quad \\text { for } l=1_1, L \\\\\nz_l \u0026 =\\varphi_l\\left(\\left[1, z_{l-1}\\right] \\cdot B_l^{(t-1)}\\right) \\quad \\text { store } \\tilde{z}_l, z_l \\text { along the way }\n\\end{aligned}$$ Backward pass: The updates $\\Delta B_l=0$, for i in batch: $$\\begin{aligned}\n\\widetilde{\\delta}_L=\\frac{\\partial l_{\\text {loss }}}{\\partial \\tilde{z}_L} \\text { for } l=L_1,..,1: \\qquad\u0026 \\Delta B_l+=\\tilde{z}_{l-1}^{\\prime} \\cdot \\tilde\\delta_l \\\\\n\u0026 \\tilde{\\delta}_{l-1}=\\tilde{\\delta}_l \\cdot\\left(B_l^{(t-1)}\\right)^{\\top} \\cdot \\operatorname{diag}\\left(\\varphi_{l-1}^\\prime\\left(\\tilde{z}_{l-1}\\right)\\right)\n\\end{aligned}$$\nUpdate: $$B_l^{(t)}=B_l^{(t-1)}-\\tau \\Delta B_l$$\n","lastmodified":"2023-10-07T08:19:48.786592134Z","tags":[]},"/notes/machine-learning/Covariance-Matrix":{"title":"Covariance Matrix","content":"","lastmodified":"2023-10-07T08:19:48.786592134Z","tags":[]},"/notes/machine-learning/Cross-Validation":{"title":"Cross Validation","content":"\n\nHow to find good parameter?\nThe Support Vector Machine has a hyper parameter: relative Weight of the data and regularization term.\n\n- regularization Term: tries to maximize the margin in order to prevent overfitting\n- data term: responsible for classifying the data term correctly\n\nQuestin: how do you find a good hyperparameter?\n\nThe standard approaches to hyperparameter selection:\n\nThe best is if you have 3 Datasets: Training, validation, test.\n\nThe idea is: \n- You pick a set of possible hyperparameters .ie $\\beta  \\in {10^{-3}, 10^{-1},1,10,100}$\n- train with each $\\beta$ on the training set \n-  measure the accuracy on the validation set\n- keep best $\\beta$ on validatoin set\n- test again on test set\n\nWhy not get rid of validation test? If we select the hyperparamter on the test set, the test set secretly becomes part of the training set and hence is no test set anymore. This is why we need a second test set that is part of the training in order to preserve the test set for real testing.\n\n## Fallback withoud validation set: cross-validation\nidea: split the training set into k pieces (\"folds\")\n\nIt is really importatn that the Test set is randomly ordered.\n\n- split up the test set into e.g 5 folds\n- use each fold as validation set in term and train on the remaining k-1 folds\n- in the second iteration use second set and train on the remaining folds etc\n- Typical values for K: 2(only used if you cant afford more than 2 times), better is 5 or 10 Folds, (N-fold: \"leave-one-out cross validation\": leave one point out and train on all the data except that one). Often one can calculate the leave one out error analytically which is very nice!\n- can also be applied hierarchically:\n\t- first take one fold out as a test set\n\t- take another one out as a validation set\n\t- train on the remaining folds\n\t- in the inner loop you would flip around the validation sets and in the outer loop you would flip around the test set\n- K-Error Estimates after we trained: Report average error and variance: The best hyperparameter is the one with the lowest average error","lastmodified":"2023-10-07T08:19:48.786592134Z","tags":[]},"/notes/machine-learning/Gaussian-Distribution-in-1-D":{"title":"Gaussian Distribution in 1-D","content":"\n$$N\\left(x \\mid \\mu, \\sigma^2\\right)=\\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\exp \\left(-\\frac{1}{2}\\left(\\frac{(x-\\mu)^2}{\\sigma^2}\\right)\\right.$$\n\n\nStandard Normal Distribution: $\\mu = 0, \\sigma^2 = 1$\n\n","lastmodified":"2023-10-07T08:19:48.786592134Z","tags":[]},"/notes/machine-learning/Linear-Discriminant-Analysis":{"title":"Linear Discriminant Analysis","content":"\n\nIdea:\n- We have two classes and we assume that the features of each class form a cluster\n- then we model each cluster as a gaussian distribution, which means we have a generative model i.e the RHS of Bayes\n- Calculate the Posterior from the bayes formula\n\nVariant 1: Quadratic Discriminant Analysis (QDA)\n- both clusters can have different shapes\n\nVariant 2: LDA\n- both clusters have the same shape\n- -\u003e LHS of Bayes reduces to a linear decision rule i.e $\\hat y = sign(x\\\\beta + b)$ \n- all the nonlinear terms cancel out\n\nFirst of all: How do get to a gaussian?\n\nClusters are assumed to be elliptic (circles are unrealistic).\n\n- Start from unit circle\n- Step 1: axis aligned scaling. Scale the unit circle with $\\lambda=\\left(\\begin{array}{ll}r_1 \u0026 0 \\\\ 0 \u0026 r_2\\end{array}\\right)$\n- Step 2: Rotation around origin\n- Step 3: Translate to the mean\n\n\n$$z = \\lambda^{-1} Q^{-1}(x-\\mu)$$\n$$z = \\lambda^{-1} Q^T(x-\\mu)$$\n ![[Gaussian Distribution in 1-D]]\n\nClusters are unit circles around the origin:\n$$N\\left(z \\mid \\mu=0, \\Sigma=1\\right)=\\frac{1}{\\sqrt{(2 \\pi)^d}} \\exp \\left(-\\frac{1}{2} z z^{\\top}\\right)$$\nwhere d is the dimension\n\n![[Multi variate Gaussian]]\n\n\nWe can write \n$$\\Sigma = Q\\lambda^2Q^T$$\nwhich is the Eigendecomposition of $\\Sigma$, with the eigenvalues $\\lambda$ and the Eigenvectors $Q$ \n\n\n---\nNow lets fit a gaussian to a cluster. We with only one cluster\n\n$$N\\left(x_i \\mid \\mu_1, \\Sigma_1\\right)=\\frac{\\exp \\left(-\\frac{1}{2}(\\mathbf{x_i}-\\boldsymbol{\\mu_1})^{\\mathrm{T}} \\boldsymbol{\\Sigma_1}^{-1}(\\mathbf{x_i}-\\boldsymbol{\\mu_1})\\right)}{\\sqrt{\\det{2\\pi\\Sigma_1}}}$$\n\nLearning Problem: Find the mean and covariance of class 1 $\\mu_1,\\Sigma_1$\n\nFundamental Principle: Choose $\\mu_1,\\Sigma_1$ such that the Training set will be a typical outcome of the resulting model. You want to choose the model such that If you draw data from the model it should look very similar to the trainin set. This is called the \n**Maximum Likelihood Principle**: The best model maximises the likelihood of the Trainin set\n\n- Second Principle: i.i.d assumption: Training instances are drawn *independently* from the trainin set, and they are *identically distributed*\nWhy is this important? We can simplify the TS distribution:\nIndependent means that the joint distribution is the product of the individual distributions. Identically distributed means that all the instances come from the same prob-distribution, so I dont need a $p_i$ \n$$p(TS) = p(x_1, \\dots, x_n)= \\prod p(x_i)$$\n\nMaximum Likelihood: \n$$ \\hat \\mu, \\hat\\Sigma = \\arg \\max_{\\mu,\\Sigma} p(TS)$$\n\nMathematically simpler: minimize the negative Logarithm of p(TS)\n\n$$  \\arg \\max_{\\mu,\\Sigma} p(TS)\\leftrightarrow  \\arg \\min_{\\mu,\\Sigma} (-\\log p(TS))$$\n\nBecause if you apply a monotic function to an optimzation problem the arg max will not change. Taking a minus takes a maximum into a minimum\n\n$$ \\hat \\mu, \\hat\\Sigma = -\\arg \\min_{\\mu,\\Sigma} \\sum_i^N\\left( \\log \\left(\\frac{1}{\\sqrt{\\det(2\\pi\\Sigma)}} \\right)- \\frac{1}{2}(x_{i}- \\mu)\\Sigma^{-1}(x_{i}- \\mu)^{T}\n\\right)$$\n\n$$ \\hat \\mu, \\hat\\Sigma = \\arg \\min_{\\mu,\\Sigma} \\sum_i^N\\left( \\frac{1}{2} \\log  \\left(\\sqrt{\\det(2\\pi\\Sigma)} \\right)+ \\frac{1}{2}(x_{i}- \\mu)\\Sigma^{-1}(x_{i}- \\mu)^{T}\n\\right)$$\nWe can get rid of the scale facter 1/2 since it just scales and has no influence on the arg min. This is now our LOSS(TS) \n\nGeneral rule:\n\n$$\\log \\det(2 \\pi \\Sigma) = \\log ((2\\pi)^{D}\\det(\\Sigma)) = \\log(2\\pi)^{D}+ \\log \\det \\Sigma$$\n\n\nDerivative Rule from Linear Algebra:\n\n$$\\frac{\\partial vAv^{T}}{\\partial v}= 2Av^T$$\n$$\\frac{\\partial LOSS(TS)}{\\partial \\mu} = \\sum_{i} \\Sigma^{-1}(x_{i}-\\mu)^T=0 $$\nMultiply with $\\Sigma$ from the left:\n$$\\sum\\limits_{i}(x_{i}-\\mu)^T=0$$\n$$\\sum\\limits_{i}x_{i}=\\sum\\limits_{i}\\mu^{T}= N \\mu^T$$\n$$\\mu = \\frac{1}{N} \\sum\\limits_{i}x_{i}$$\nThe optimal $\\mu$ is the average. This is a mathematical proof that the average maximizes the likelihood of the data.\n\n$$\\frac{\\partial LOSS(TS)}{\\partial \\Sigma}$$ is inconvient.\n\nThe first term is rather easy\n\n$$\\frac{\\partial \\log\\det\\Sigma}{\\partial \\Sigma} = -(\\Sigma^{T)^{-1}}= -\\Sigma^{-1} $$\nsince $\\Sigma$ is symmetric.\n\nIntroduce precision matrix \n$$K = \\Sigma^{-1}$$\nCalculate:\n$$\\frac{\\partial}{\\partial K}  \\sum\\limits_i^{N}(x_i-\\mu)K(x_{i}-\\mu)^{T} = \\sum (x_{i}-\\mu)^{T} (x_{i}-\\mu)$$\nWhere the last term is called the scalar matrix.\n\nPut everything together:\n(kein bock mehr zu schreiben) Lecuture session 5,2023-05-02 Minute 1:01:11\n\n\n\n\n\n# Why is LDA a linear classifier?\nPriors:\n\n$$\np(y=1)=\\frac{N_1}{N} \\quad p(y=-1)=\\frac{N_{-1}}{N}\n$$\nassume for simplicity that these two are the same .i.e 1/2\n\nThe Gaussian for class 1 is given by\n$$\np(x | y=1)=\\frac{1}{\\sqrt{\\operatorname{det}(2 \\pi \\Sigma)}} \\exp \\left(-\\frac{1}{2}\\left(x- \\mu_1\\right) \\Sigma^{-1}(x-\\mu_1)^{\\top}\\right)\n$$\nThe decision rule is then given by the argmax of the posterior probability\n\n$$\n\\hat{y}_i=\\operatorname{argmax}_k p\\left(Y=k \\mid x_i\\right)\n$$\ninserting bayes rule gives\n\n$$\n\\hat{y}_i=\\operatorname{argmax}_{k}\\frac{p\\left(x_i \\mid y=k\\right) p\\left(y=k\\right)}{\\sum_{k^{\\prime}} p\\left(x_i \\mid y=k^{\\prime}\\right) p\\left(y=k^{\\prime}\\right)}=\\begin{cases}1 \u0026 \\text { if } p\\left(y=1 \\mid x_i\\right)\u003e\\frac{1}{2} \\\\ -1 \u0026 \\text { if } p\\left(y=1| x_i\\right)\u003c\\frac{1}{2} \\\\ \u0026 \\Leftrightarrow p\\left(y =-1 \\mid x_i\\right)\u003e\\frac{1}{2}\\end{cases}\n$$\n\n\nCalculate posterior analytically:\n$$\np(y=1 \\mid x)=\\frac{p(x \\mid y=1) p(y=1)}{p(x \\mid y=1) p(y=1)+p(x \\mid y=-1) \\mid p(y=-1)}\n$$\n$$=\\frac{p(x \\mid y=1)}{p(x \\mid y=1)+p \\mid(x \\mid y=-1)}$$\n$$\\begin{equation}\n\\begin{aligned}\n\u0026 =\\frac{1}{1+\\frac{p(x \\mid y=-1)}{p(x \\mid y=1)}} \\\\\n\u0026 \\frac{p(x \\mid y=-1)}{p(x \\mid y=1)}=\\exp \\left(-x \\Sigma^{-1}\\left(\\mu_1^{\\top}-\\mu_{-1}^{\\top}\\right)-\\frac{1}{2}\\left(\\mu_{-1} \\Sigma^{-1} \\mu_{-1}^{\\top}-\\mu_1 \\Sigma^{-1} \\mu_1^T\\right)\\right)=\\exp (-(x \\beta+b))\n\\end{aligned}\n\\end{equation}$$\n\n\n\nSo we get:\n$$\np(y=1\\mid x)=\\frac{1}{1+\\operatorname{exp}(-(x \\beta+b))}\n$$\nwhich is called the (logistic) sigmoid function\n![[Unbenannt.png]]\n\n\nThis satisfies:\n$$\n\\sigma(-t)=1-\\sigma(t)\n$$\n$$\n\\frac{d}{dt}\\sigma(t)=\\sigma(t) \\cdot \\sigma(-t)\n$$\n\nSo the posterior is simply\n$$\np(y=1 \\mid x)=\\sigma(x \\beta+b)\n$$\n$$\np(y=-1 \\mid x)=\\sigma(-(x \\beta+b))\n$$\nwhere \n$$\n\\begin{aligned}\n\u0026 \\beta=\\Sigma^{-1}\\left(\\mu_1^{\\top}-\\mu_{-1}^{\\top}\\right) \\\\\n\u0026 \\left.b=\\frac{1}{2}\\left(\\mu_{-1}\\Sigma^{-1} \\mu_{-1}^{\\top}-\\mu_1 \\Sigma^{-1} \\mu_{1}^{\\top}\\right)\\right)\n\\end{aligned}\n$$\nSo the decision rule is\n$$\n\\hat{y}=\\underset{k}{\\operatorname{argmax}} p(y=k \\mid x)\n$$\n$$\n\\Leftrightarrow\\left\\{\\begin{array}{lll}\n1 \u0026 \\text { if } \u0026  \\sigma(x \\beta+b)\u003e\\frac{1}{2} \\\\\n-1 \u0026 \\text { if } \u0026 \\sigma(-(x \\beta+b))\u003e\\frac{1}{2}\n\\end{array}\\right.\n$$\n$$\n\\Leftrightarrow\\left\\{\\begin{array}{lll}\n1 \u0026 \\text { if } \u0026 x \\beta+b\u003e0 \\\\\n-1 \u0026 \\text { if } \u0026 x \\beta+b\u003c0\n\\end{array}\\right.\n$$\n$$\n\\hat{y}=\\operatorname{sign}(x\\beta+b)\n$$\n\nWhich is linear!!!\n\n","lastmodified":"2023-10-07T08:19:48.786592134Z","tags":[]},"/notes/machine-learning/Logisitic-Regression":{"title":"Logisitic Regression","content":"\nUse the same posterior as [[Linear Discriminant Analysis]], but train LHS of Bayes rule, which gives a different solution.\n\n\n- i.i.d assumption: all labels are drawn independently from same posterior\n\n\n$$p((Y_{i}^{*})_{i=1}^{N} \\mid (X_{i})_{i=1}^{N})=\\prod_{i=0}^Np(Y_i^{*} \\mid X_i)$$\n- Maximum Likelihood Principle: choose Parameters such that posterior of TS is maximized\n$$\\hat{\\beta},\\hat{b} =\\text { argmax }_{\\beta, b} \\prod_{i=1}^{N} p\\left(Y_{i}^{*}\\mid X_{i}\\right)$$ $$=\\arg \\min-\\sum\\limits_{i}^{N}\\log p\\left(Y_{i}^{*}\\mid X_{i}\\right)= \\arg \\min \\sum\\limits_{i: Y_{i}^{*}=1} \\sigma(x\\beta+b)-\\sum\\limits_{i: Y_{i}^{*}=-1} \\sigma(-(x\\beta+b))$$\n\n\nwhich gives\n\n$$\\hat\\beta,\\hat b=\\arg \\min_{\\beta,b}-\\sum\\limits_{i=1}^{N}\\sum\\limits_{k}1[Y_{i}^{*}=k] \\log \\sigma_k(x\\beta+b)$$\n\n\nWhat is the logarithm of the sigmoid?\n![[Softplus Function]]\n\n\nCommon convention in literature is to write the labels as\n$$y_i^* \\in\\{0,1\\}$$\nThen the Logistic Regression Objective is\n\n\n$$\\hat{\\beta}, \\hat{b}=\\arg \\min_{\\beta, s}-\\sum_{i=1}^N\\left[Y_{ i }^{*}\\log \\sigma\\left(x \\beta+b)+\\left(1-Y_i^*\\right) \\log \\sigma(-(x \\beta+b))]\\right.\\right.$$\n\nThis has no analytic solution, but it is a convex objective, which means that iterative algorithms have unique solutions\nSo we make a simplifaction that if the features are centered, we can set $b=0$\n\nThe derivatives are:\n$$\n\\frac{\\partial \\sigma(x \\beta)}{\\partial \\beta}=\\sigma^{\\prime}(X \\beta) \\cdot x=\\sigma(X \\beta) \\cdot \\sigma(- x \\beta) \\cdot x\n$$\n$$\n\\frac{\\partial \\log \\sigma(x \\beta)}{\\partial \\beta}=\\frac{1}{\\sigma(x \\beta)} \\sigma(x \\beta) \\sigma \\sigma(-x \\beta) \\cdot x = \\sigma(-x\\beta)x\n$$\n$$\n\\frac{\\partial \\log \\sigma(-x \\beta)}{\\partial \\beta}=\\frac{1}{\\sigma(-x \\beta)} \\sigma(x \\beta) \\sigma \\sigma(-x \\beta) \\cdot (-x) = -\\sigma(x\\beta)x\n$$\nFrom which we can derive the derivate of the entire Training set\n\n$$\n\\frac{\\partial \\text { loss }(T S)}{\\partial \\beta}=-\\sum_{n=1}^N\\left[Y_{i}^{*} \\sigma\\left(-x_i \\beta\\right) x_i-\\left(1-Y_i^*\\right) \\sigma\\left(x_i \\beta\\right)\\left(-x_i\\right)\\right]\n$$\n\n\nWhich gives after simplifying \n$$\n\\frac{\\partial\\operatorname{loss}(\\partial \\beta)}{\\sigma_i}=\\sum_{i=1}^N\\left(\\sigma\\left(x_i \\beta\\right)-Y_i^*\\right) x_i \\quad \\stackrel{!}{=} 0\n$$\nThis explains what is happening: the term in the sum is basically the error\n\n- case 1: $Y_{i}^{*}=1$ and classifier correct: $\\sigma(X_i\\beta)=1$ -\u003e Error is close to 0\n-  case 2: $Y_{i}^{*}=1$ and classifier wrong: $\\sigma(X_i\\beta)\\approx 0$ -\u003e Error is close to -1 -\u003e so because we do gradient decent, this means we move $\\sigma(X_{i}\\beta)\\rightarrow 1$\n-  case 3: $Y_{i}^{*}=0$ and classifier correct: $\\sigma(X_i\\beta)=0$ -\u003e error is clos to 0, no correction\n-  case 4: $Y_{i}^{*}=0$ and classifier wrong: $\\sigma(X_i\\beta)=1$ -\u003e Error close to 1 --\u003e Correction (gradient descent) towards $\\sigma(X_{i}\\beta)\\rightarrow 0$\n","lastmodified":"2023-10-07T08:19:48.786592134Z","tags":[]},"/notes/machine-learning/Machine-Learning-Essentials":{"title":"Machine Learning Essentials","content":"\nHere are my Lecture Notes of the Machine Learning Essentials Lecture by Ullrich K√∂the , SS 2023, Heidelberg.\n- 02.05.2023:\n\t- [[Cross Validation]]\n\t- [[Linear Discriminant Analysis]]\n\t- [[notes/machine-learning/Alternatives for learning beta and b]]\n- 09.05.23 Session 8\n\t- [[Summary Linear Classification]]\n\t- [[Multi Class Classification]]\n\t- [[Non Linear Classification]]\n\t- [[Neural Networks]]\n- 12.05: \n\t- [[Backpropagation]]\n\t- \n\n\n","lastmodified":"2023-10-07T08:19:48.786592134Z","tags":["ml"]},"/notes/machine-learning/Mode-Collapse":{"title":"Mode Collapse","content":"\nCopied from: [https://machinelearning.wtf/terms/mode-collapse/]\n\n---\n\n\n\n**Mode collapse**, also known as **catastrophic collapse** or **the Helvetica scenario**, is a common problem when training [generative adversarial networks (GANs)](https://machinelearning.wtf/terms/generative-adversarial-network-gan/).\n\nA GAN is a framework for creatively and artificially generating novel data samples, structured as a zero-sum (minimax) game between two neural networks:\n\n-   a **discriminator** network that distinguishes between real-world data samples and artificially generated imitation data samples.\n-   a **generator** network that learns to create artificial data that is realistic enough to fool the discriminator.\n\nA GAN is successfully trained when both of these goals are achieved:\n\n1.  The generator can reliably generate data that fools the discriminator.\n2.  The generator generates data samples that are as diverse as the distribution of the real-world data.\n\n**Mode collapse** happens when the generator fails to achieve Goal #2‚Äìand all of the generated samples are very similar or even identical.\n\nThe generator may ‚Äúwin‚Äù by creating one realistic data sample that always fools the discriminator‚Äìachieving Goal #1 by sacrificing Goal #2.\n\n**Partial mode collapse** happens when the generator produces realistic and diverse samples, but obviously much less diverse than the real-world data distribution. For example, when training a GAN to generate human faces, the generator might succeed in producing a diverse set of male faces but fail to generate any female faces.\n\n---\n\n## Solutions to mode collapase\n\n-   **[Wasserstein loss](https://arxiv.org/abs/1701.07875).** Formulates the GAN loss functions to more directly represent minimizing the distance between two probability distributions. The Wasserstein loss is designed to fix an issues caused by the original GAN loss functions being designed as a zero-sum (minimax) game. Problems like mode collapse happen because the generator winning a turn in the game does not correlate with actually reducing the distances between the generated and real-world probability distributions.\n-   **[Unrolling](https://arxiv.org/abs/1611.02163).** Updating the generator‚Äôs loss function to backpropagate through k\n\nsteps of gradient updates for the discriminator. This lets the generator ‚Äúsee‚Äù k-   steps into the ‚Äúfuture‚Äù‚Äìwhich hopefully encourages the generator to produce more diverse and realistic samples.\n-   **[Packing](https://arxiv.org/abs/1712.04086).** Modifying the discriminator to make decisions based on multiple samples all of the same class‚Äìeither real or artificial. When the discriminator looks at a pack of samples at once, it has a better chance of identifying an un-diverse pack as artificial.","lastmodified":"2023-10-07T08:19:48.786592134Z","tags":[]},"/notes/machine-learning/Multi-Class-Classification":{"title":"Multi Class Classification","content":"\nTwo Possibilites:\n\n- reduce to a set of 2-class problems\n- or: use a more powerful model\n\n\n# One- against-rest classification\n\nFor each class $k=1,..,c$ define $\\beta_4 b_k \\Rightarrow$ score $s_k=X_i \\beta_k+b_k$\nThese are then trained by treating $y_{i}^{*}=k$ as \"Class +1\" and $y_{i}^{*}\\neq k$ as \"Class -1\" i.e the rest. So we train $c$ classifiers in total.\n\nThen we need to make them comparable by normalizatoin\n$\\hat{\\beta}_4=\\frac{\\beta_k}{\\left\\|\\beta_k\\right\\|}, \\hat{b}_k=\\frac{b_k}{\\left\\|\\beta_k\\right\\|}$\n\n- Classify according to the biggest score, or \"dont know\" if all scores are bad\n\nso the decision rule is\n\n$$\n\\hat{y}_i=\\left\\{\\begin{array}{l}\n\\text { \"unkmon\" if } s_k\u003c\\varepsilon \\forall k \\\\\n\\operatorname{argmax}_k s_k\n\\end{array}\\right.\n$$\n\n\n# all-pairs\ntrain a linear model for all $k,k'$ which is $c(c-1)/2$ models in total\nScores for all pairs\n$$\ns_{kk^{\\prime}}=x_i \\beta_{k k^{\\prime}}+b_{k k^{\\prime}} \\text { for all } k^{\\prime} \\neq k\n$$\nif $s_{kk^{\\prime}}\u003e0 \\rightarrow$ one vote for class k, if $s_{kk^{\\prime}}\u003c0 \\rightarrow$ one vote for class k'\n\nIn the end:\n$$\\hat y_{i}= \\text{label with most votes or \"unkown if all k received about equally many\"}$$\n\n\n\n\n# Define posterior as sofrmax funtion\n![[Softmax Function]]\n\nstandard for neural network classification\n\nNow: train all $\\beta_k,b_k$ jointly i.e together at some time\n\nWe use the Loss function \n\n![[cross-entropy loss]]","lastmodified":"2023-10-07T08:19:48.786592134Z","tags":[]},"/notes/machine-learning/Multi-variate-Gaussian":{"title":"Multi variate Gaussian","content":"\n$$N\\left(x \\mid \\mu, \\Sigma\\right)=\\frac{\\exp \\left(-\\frac{1}{2}(\\mathbf{x}-\\boldsymbol{\\mu})^{\\mathrm{T}} \\boldsymbol{\\Sigma}^{-1}(\\mathbf{x}-\\boldsymbol{\\mu})\\right)}{\\sqrt{\\det{2\\pi\\Sigma}}}$$","lastmodified":"2023-10-07T08:19:48.786592134Z","tags":[]},"/notes/machine-learning/Neural-Networks":{"title":"Neural Networks","content":"\nDefinition of a Neuron:\n\n- inputs $Z_{in}$ (row vector)\n- computation: Linear function followed by non-linearity $$Z_{out}=\\varphi(Z_{in}\\cdot\\beta+b)$$ which we call the activation of a Neuron. We call $Z'=Z_{in}\\cdot\\beta+b$ the pre-activation. Why do we need this Non-linearities? Because with out them the whole Neural Network reduces to one single Linear Transformation, since a concatenation of Linear Transformation is again simply a linear Transfromation\n- There are popular activation functions $\\varphi(Z')$\n\t- identity function: $\\varphi(Z')=Z'$ is linear. Used as output of regression networks\n\t- classic choice: \n\t\t- $\\varphi(Z')= \\sigma(Z')$ (sigmoid)\n\t\t- $\\varphi(Z')= \\tanh(Z')$ \n\t- modern:\n\t\t- $\\varphi(Z')= \\operatorname{ReLu}(Z')$  despite its non differentiable at 0 it performs better than the sigmoid function\n\t\t- $$\\varphi(Z')= \\operatorname{LeakyReLu}(Z')=\\left\\{\\begin{array}{l}\nZ^{\\prime}\\quad\\text{if} \\quad Z^{\\prime}\u003e0 \\\\\n\\alpha Z^{\\prime}\\quad\\text{if} \\quad Z^{\\prime}\u003c0,\\alpha\u003e0\n\\end{array}\\right.$$Where $\\alpha$ is another hyperparameter or is learned\n- exponential linear unit:$$\n\\varphi\\left(z^{\\prime}\\right)=E L U\\left(z^{\\prime}\\right)=\\left\\{\\begin{array}{l}\nz^{\\prime} \\quad\\text{if} \\quad z^{\\prime}\u003e0 \\\\\n\\alpha\\left(e^{z^{\\prime}-1}\\right) \\quad\\text{if} \\quad z^{\\prime}\u003c0,\n\\end{array}\\right.\n$$\n- swish function:$$\n\\text { Swish }\\left(z^{\\prime}\\right)=z^{\\prime} \\cdot \\sigma\\left(z^{\\prime}\\right)\n$$\n\n\n\n---\n\nA Neural Network is simple created by connecting many Neurons \n - in parallel: these neurons form a layer\n - in series: \"deep\", means that we have many layers\n\nBasic Network type: fully connected, each Neuron is connected with every neuron on the next layer\n\nIn around 2005, People discovered that GPUs could implement these Networks really efficiently, which enabled much bigger networks and improve results. Additionally, big data became available for the first time, and you need big data in order to train a big network.\n\n--- \n## Activations Functions\n\nThe activation functions $\\varphi_1,...,\\varphi_{L-1}$ (where $L$ is the number of layers not counting the input) are chosen by the designer (\"hyperparamter\"), and is usually ReLU or ELU or swish.\n\nThe last layer $\\varphi_L$ is determined by application:\n- regression ($Y\\in \\mathbb{R}$ ): identity\n- classification ($\\left.p(Y=k \\mid x\\right))$ where $Y$ is a vector: softmax\n\nPossible interpretation: \n- $Z_{L-1}=\\psi([1,X])$: non-linear transformation of features (learned!) to fulfill requirements of last layer algorithm. The $\\psi$ function is the entire network until the last layer. \n- $Z_L=\\varphi_L\\left(\\left[1, Z_{L-1}\\right] \\cdot \\beta_L\\right)$: classical algorithm, e.g least squares regression, logistic regression(for classification)\n\n\n# Important theorems\n\n1. **Neural Networks with at least 2 layers are universal approximators**: This means that if the Network is big enough i.e we have enough neurons in the hidden layer, we can approximate **any** function to **any** desired accuracy. But: This is a purely existential proof: It says that the network exists, but it doesnt say anything about how to find (train) it. (\"The solution exists but we have no idea how to compute it\")\n2. **Finding the optimal network is NP-hard (takes exponential time in network size)**: If you make your network bigger you have a lower chance to finding the optimal parameters, so you need approximation algorithms are needed.\n\nThe error people did up to 2005 was that since 2-layers are sufficient, only use 2-layer nets. But it was found that deeper networks are easier to train since the probability that you end up in a good local optimum is essentially 1\n\n\nTrainint of these Networks is done by [[Backpropagation]]\n\n\n","lastmodified":"2023-10-07T08:19:48.786592134Z","tags":[]},"/notes/machine-learning/Non-Linear-Classification":{"title":"Non Linear Classification","content":"\nUsed when Data is not Linear separable\n\n0. Bad Practise: Use LR and hope for the best. Exception: Very sparse data, when its hard to justify non-linear fit\n1. Measure more features, increase $D$ or measure $x_i$ more accuratly. higher dimensional spaces tend to be more linearaly separable, but might be overfitting. There is a Theory that claims that $N=D+1$ is always seperable\n\nif $D\u003eN$: use sparse learning methods: (Automatically select for important features and ignore the rest e.g LASSO regression)\n\n2. Use non-linear classifier: e.g QDA (quadratic discriminant Analysis) is like LDA but seperate covariance $\\Sigma_k$ for each $k$ , which gives a curved decision boundary.\n\tPredict by the RHS of Bayes Formule\n\n\t- kernel-SVM: non-linear generelization of SVM\n\t- Decision trees/forests: recursively subdivide the x-space\n\n3. non-linearly transform features $\\tilde X_i = \\phi(x_i)$ and choose the transformation such that data are linearly seperable in $\\tilde X_i$ space.  The problem is  that handcraftign the function $\\phi$ is difficult and time consuming. The Solution is to lean the function using multi layer neural networks. So if you have L layers, the layers L,... L-1 implement the function $\\phi$ and the last Layer L is a linear classifier (Usually LR)","lastmodified":"2023-10-07T08:19:48.786592134Z","tags":[]},"/notes/machine-learning/Normalizing-Flows":{"title":"Normalizing Flows","content":"This is a summary of Papamakarios et al. 2021 https://arxiv.org/abs/1912.02762\n\n# Introduction to Normalizing Flows\n\n\nA normalizing flow simply transforms a simple densty multiple times to produce a richer and more complicated distribution. So the goal is to find the right function that right functions that transform a simple gaussian distribution such that the transformed sample follows exactly the desired distribution.\n\nBut how do we find such function? It is very complicated task, since this transformations are not trivial and we expect the transformation function to be extremly complex. So we use Machine Learning!\n\n\n## Definition and Basics\n\nSuppose you have a $D$ dimensional real vector $x$. The main goal is to express $x$ as a transformation of a real vector $u$ sampled from a distribution $p_u(u)$, meaning\n$$\\mathbf{x}=T(\\mathbf{u}) \\text { where } \\quad \\mathbf{u} \\sim p_{\\mathrm{u}}(\\mathbf{u})$$\nwe call $p_u(u)$ the **base distribution** of the flow based model. The transformation $T$ needs to have special properties:\n- $T$ must be invertible\n- $T$ and $T^{-1}$ must be differentiable\n\n\u003e [!note]\n\u003e  Invertible and differentiable transformations are composable:\n\u003e  $$\\begin{aligned}\n\\left(T_2 \\circ T_1\\right)^{-1} \u0026 =T_1^{-1} \\circ T_2^{-1} \\\\\n\\operatorname{det} J_{T_2 \\circ T_1}(\\mathbf{u}) \u0026 =\\operatorname{det} J_{T_2}\\left(T_1(\\mathbf{u})\\right) \\cdot \\operatorname{det} J_{T_1}(\\mathbf{u}) .\n\\end{aligned}$$\n\n\nSuch transformation are known as [[diffeomorphisms]], and they require that $u$ be $D$-dimensional as well. It follows that the density of $x$ is well defined and can be calculated by the change of variables:\n$$\np_{\\mathrm{x}}(\\mathbf{x})=p_{\\mathrm{u}}(\\mathbf{u})\\left|\\operatorname{det} J_T(\\mathbf{u})\\right|^{-1} \\quad \\text { where } \\quad \\mathbf{u}=T^{-1}(\\mathbf{x})\n$$\nEquivalently, we can also write $p_{\\mathrm{x}}(\\mathbf{x})$ in terms of the Jacobian of $T^{-1}$ :\n$$\np_{\\mathrm{x}}(\\mathbf{x})=p_{\\mathrm{u}}\\left(T^{-1}(\\mathbf{x})\\right)\\left|\\operatorname{det} J_{T^{-1}}(\\mathbf{x})\\right|\n$$\nThe Jacobian $J_T(\\mathbf{u})$ is the $D \\times D$ matrix of all partial derivatives of $T$ given by:\n$$\nJ_T(\\mathbf{u})=\\left[\\begin{array}{ccc}\n\\frac{\\partial T_1}{\\partial \\mathrm{u}_1} \u0026 \\cdots \u0026 \\frac{\\partial T_1}{\\partial \\mathrm{u}_D} \\\\\n\\vdots \u0026 \\ddots \u0026 \\vdots \\\\\n\\frac{\\partial T_D}{\\partial \\mathrm{u}_1} \u0026 \\cdots \u0026 \\frac{\\partial T_D}{\\partial \\mathrm{u}_D}\n\\end{array}\\right]\n$$\n\u003e [!note]\n\u003e  The absolute of the Jacobi determinant quantifies the change of volume due to $T$. Since the probability mass in $dx$ must equal the probability mass in $du$, so if $du$ is expanded, the density at $x$ is be smaller than the density at $u$ and vice versa.\n\nThe total transformation is obtained by chaining together multiple transformation $$\nT=T_K \\circ \\cdots \\circ T_1,$$\nwhere each $T_k$ transforms $\\mathbf{z}_{k-1}$ into $\\mathbf{z}_k$ assuming $\\mathbf{z}_0=\\mathbf{u}$ and $\\mathbf{z}_K=\\mathbf{x}$\n\nSo a flow based model provides:\n- sampling: $$\n\\mathbf{x}=T(\\mathbf{u}) \\text { where } \\quad \\mathbf{u} \\sim p_{\\mathrm{u}}(\\mathbf{u})\n$$\n- evaluation: $$\np_{\\mathrm{x}}(\\mathbf{x})=p_{\\mathrm{u}}(\\mathbf{u})\\left|\\operatorname{det} J_T(\\mathbf{u})\\right|^{-1} \\quad \\text { where } \\quad \\mathbf{u}=T^{-1}(\\mathbf{x})\n$$\n## Can Normalizing Flows represent any distribution?\n\nThe first quesetion that may come up is: is it always possible to start from a simple distribution and transforming it to any distribution $p_X(x)$?\n\nIn other words: We want to prove that for any pair of well-behaved distribution $p_x(x)$ (target) and $p_u(u)$ (base) there exists a diffeomorphism that can turn $p_u(u)$ into $p_X(x)$.\n\n(proof?)\n\n## Fitting flow-based models\nFitting a flow based model $p_X(x;\\theta)$ to a target distribution $p_x^*(x)$ is done by minimizing some kind of divergence or discrepancy between them. The minimization is performed w.rt the parameters  of the model $$\\boldsymbol{\\theta}=\\{\\phi, \\psi\\}$$\nwhere $\\phi$ are the parameters of the transformation $T$ and $\\psi$ are the parameters of $p_u(u)$. The most popular divergence for fitting flow-based models is the **Kullback-Leibler (KL)** divergence.\n\n\n\n# Constructing Flows\n## Finite Compositions\nAs already discussed we can compose flows with finite number of simple transformations $T_k$ \n$$T=T_K \\circ \\cdots \\circ T_1$$\nThis way we can use simple transformation, which each have a tractable inverse and jacobi determinant, to construct a complex transformation.\n\u003e [!note]\n\u003e Tractable Jacobian Determinant: We can always compute a jacobian matrix of a differentiable function with $D$ inputs and $D$ outputs using $D$ passes of either forward or reverse mode automatic differentiation. This computation results in a time cost of $\\mathcal{O}\\left(D^3\\right)$ which can be intractable for large D. For flow based models we expect the jacobion detereminant computation to be at most linear in $D$.\n\nSo by assuming $z_0 = u$ and $z_K = x$ the forward evaluation is \n$$\\mathbf{z}_k=T_k\\left(\\mathbf{z}_{k-1}\\right) \\quad \\text { for } k=1, \\ldots, K$$\nthe inverse evaluation is:\n$$\\mathbf{z}_{k-1}=T_k^{-1}\\left(\\mathbf{z}_k\\right) \\quad \\text { for } k=K, \\ldots, 1$$ \nand the Jacobian-determinant computation (in the log domain) is:\n$$\\log \\left|\\operatorname{det} J_T\\left(\\mathbf{z}_0\\right)\\right|=\\log \\left|\\prod_{k=1}^K \\operatorname{det} J_{T_k}\\left(\\mathbf{z}_{k-1}\\right)\\right|=\\sum_{k=1}^K \\log \\left|\\operatorname{det} J_{T_k}\\left(\\mathbf{z}_{k-1}\\right)\\right|$$\nThe computational complexity grows like $\\mathcal{O}(K)$ with increasing depth i.e number of composed sub-flows.\n\n\nWe implement $T_k$ or $T_k^{-1}$ using a neural network with parameters $\\phi_k$ which we denote as $f_{\\phi_k}$, which either take $z_{k-1}$ as an input and output $z_k$ ($T_k$ implementation), or the other way round. Nevertheless, we need to ensure that the model is invertible and has a tractable Jacobian determinant.\n\n\u003e [!note]\n\u003e Enuring that $f_{\\phi_k}$ is invertible and explicitly calculating its inverse are not(!) the same! Since even if we know that the inverse exists, it may be that it can be expensive or even intractable to compute it exactly.\n\n","lastmodified":"2023-10-07T08:19:48.786592134Z","tags":[]},"/notes/machine-learning/Softmax-Function":{"title":"Softmax Function","content":"\n\n$$\np\\left(\\hat{Y}_i=k \\mid X_i\\right)=\\frac{\\exp \\left(s_k\\right)}{\\sum_{\\dot{k}^{\\prime}=1}^c \\exp \\left(s_{k^{\\prime}}\\right)}=\\operatorname{softmax}(s_i,..,s_c)\n$$\n","lastmodified":"2023-10-07T08:19:48.786592134Z","tags":[]},"/notes/machine-learning/Softplus-Function":{"title":"Softplus Function","content":"\n\n$$\n-\\log \\sigma(t)=-\\log \\frac{1}{1+\\exp (-t)}=\\log (1+\\exp (-t))\n$$\nIs like a smoothed Version of ReLu","lastmodified":"2023-10-07T08:19:48.786592134Z","tags":[]},"/notes/machine-learning/Summary-Linear-Classification":{"title":"Summary Linear Classification","content":"# Summary of Linear Classification\nwith $y \\in\\{-1,1\\}$ : All methods have the same decision Rule\n\n$$\n\\hat{y}_i=\\operatorname{sign}\\left(x_i \\cdot \\beta+b\\right)\n$$\n\nBut: Methods differ by how they define and find the optimal $\\beta$ and $b$ \nthe common objective function is to have \n\n$$\n\\hat{\\beta}, \\hat{b}=\\arg \\min _{\\beta, b} \\frac{\\lambda}{2} \\beta^T \\beta+\\frac{1}{N} \\sum_{i=1}^N \\operatorname{loss}\\left(y_i^*, x_i, \\beta+b\\right)\n$$\nWhere the first term is the **Regularization** Term, and the second is called the **Data Term**\n- Data Term: takes care that we solve the right problem\n- Regularization prevents overfitting\n\nThe Methods differ by regularization and loss\n\nWe had 4 Methods:\n- Perceptron:\n\t- Loss: $\\operatorname{ReLu}\\left(\\left(-y_i^*\\left(x_i \\beta+b\\right)\\right)\\right.$\n\t- regularization: $\\lambda =0$\n- SVM:\n\t- Loss: $\\operatorname{ReLu}\\left(\\left(1-y_i^*\\left(x_i \\beta+b\\right)\\right)\\right.$\n\t-  regularization: $\\lambda \u003e0$\n \n- LDA\n\t- Loss: $\\left(y_i^{*}-\\left(x_i \\beta+b\\right)\\right)^2$\n\t-  regularization: $\\lambda =0$ if we fit $\\mu,\\sigma$ and $\\lambda\u003e0$ if we fit via objective\n- Logistic Regression (LR)\n\t- $\\operatorname{softplus}\\left(-y_i^*\\left(x_i \\beta+(s)\\right)\\right)$\n\t- regularizaizon: either $\\lambda = 0$ or $\\lambda\u003e0$\n\t\t- if you find that the LR overfits you can add the regularizazion term\n\n\nIn practice:\nSimilar solutions when data are nearly linearly seperable and different tradeoffs otherwise. So you have to check with a validation test. IMPORTANT: you should NOT check with the test set, because selecting an algorithm is part of training, and if you use the test data for selecting an algortithm it would become part of the training data and you have no test data anymore","lastmodified":"2023-10-07T08:19:48.786592134Z","tags":[]},"/notes/machine-learning/Variational-Inference-with-Normalizing-Flows-Paper":{"title":"Variational Inference with Normalizing Flows Paper","content":"\n\nhttps://arxiv.org/pdf/1505.05770v6.pdf\n\n\n# Fragen:\n-  What is variational Inference?\n- Why is posterior approximation in variational inference a disadvantage\n- inferential and variational methods difference\n- \n\n\n\n\n\n\n\n\n# Notes on Paper:\n- variational methods have huge succes and ongoing advances, but have a number of disadvantges. One main limitation is the choice of posterior approximation that is adressed in the paper\n- Variational Inferences requires that intractable posterior distributions be approximated by class of known Probability distributions, over which we search for the true posterior\n\t- class of approximations is limited: i.e mean field approximation, implies that no solution is ever able to resemble true posterior distribution\n- Why are we interested in richer more faithful posterior approximations\n\t- evidence that it results in better performance.\n- Proposals for rich posterior approximations:\n\t- \n","lastmodified":"2023-10-07T08:19:48.786592134Z","tags":[]},"/notes/machine-learning/Youtube-Tutorial-Brubaker-Normalizing-Flows":{"title":"Youtube Tutorial Brubaker Normalizing Flows","content":"\nhttps://www.youtube.com/watch?v=u3vVyFVU_lI\u0026ab_channel=MarcusBrubaker\n\n\n\n# What are Normalizing Flows?\n\n- Probabilistic Generative Model built on invertible transformaitons\n- They are Generally:\n\t- Efficient to sample from $p_X(x)$\n\t- Efficient to evaulate $p_X(x)$ almost exactly\n\t- highly (flexibly) expressive\n\t- useful latent representation\n\t- straightforward to train\n\n\n# What are NF now mathematically?\n\nbasically just an application of the change of variables formula\n$$p_{x(x)}= p_z(f(x))|det Df(x)|$$\nwhere\n- Z=f(X) is an invertible and differentiable $f(x)$ transformation\n- Volume correction term: $Df(x)$ is the Jacobian\n\n## Basic Idea of Normalizing Flows:\nAssume we have a given X which are e.g images. We want to choose an convenient, easy to work with distribution of Z, $p_Z(Z)$\nWhat normalizing flow does given this formula it tries to find the function $f(x)$ that transforms the very complex distribution over X into an nice simple distribution of Z. We want the distribution of images look like simple gaussian noise.\n\ni.e\n\nLearn $f(x)$ to transform data distribution $p_X(x)$ into $p_Z(z)$\n\ntwo main pieces:\n- Base Measure: $p_Z(z)$ is typically a standard normal distribution\n- Flow $f(x)$ must be invertible and differentiable\n\n\n\nTraining is mostly done via maximum log likelihood\n$$\\max_\\theta\\sum\\limits_{i}^{N} \\log p_{z}(f(x_{i}| \\theta ))+ \\log |\\det Df(x_{i}|\\theta)|$$\n","lastmodified":"2023-10-07T08:19:48.786592134Z","tags":[]},"/notes/machine-learning/cross-entropy-loss":{"title":"cross-entropy loss","content":"?\n\n\n\nSolve by gradient decent since it has no analytic solution\n","lastmodified":"2023-10-07T08:19:48.786592134Z","tags":[]},"/notes/physics/Navier-Stokes-Equations":{"title":"Navier-Stokes Equations","content":"\n$$\n\\frac{\\partial \\vec{v}}{\\partial t}+(\\vec{v} \\cdot \\vec{\\nabla}) \\vec{v}=-\\frac{1}{\\rho} \\vec{\\nabla} p-2(\\vec{\\Omega} \\times \\vec{v})+\\nu \\Delta \\vec{v}+\\vec{g}\n$$\n\n- $\\frac{\\partial \\vec{v}}{\\partial t}$: This term represents the partial derivative of the velocity vector (\\vec{v}) with respect to time (\\partial t). It captures the change in velocity over time and represents the acceleration or deceleration of the fluid particles.\n\n- $(\\vec{v} \\cdot \\vec{\\nabla}) \\vec{v}$: This term corresponds to the convective or advective term. It involves the dot product (\\cdot) between the velocity vector (\\vec{v}) and the gradient operator (\\vec{\\nabla}), followed by another dot product with the velocity vector (\\vec{v}). It describes the effect of the fluid's own velocity on the acceleration of neighboring fluid particles and represents the transport of momentum within the flow.\n\n- -$\\frac{1}{\\rho} \\vec{\\nabla} p:$ This term represents the pressure gradient in the fluid. It involves the gradient operator (\\vec{\\nabla}) acting on the scalar pressure field (p). Dividing by the density (\\rho) accounts for the influence of pressure on the acceleration of the fluid particles.\n\n- $2(\\vec{\\Omega} \\times \\vec{v})$: This term represents the Coriolis force. It involves the cross product (\\times) between the angular velocity vector (\\vec{\\Omega}) and the velocity vector (\\vec{v}). It captures the effect of the rotation of the coordinate system on the fluid flow.\n\n- $\\nu \\Delta \\vec{v}$: This term represents the viscous forces in the fluid. It involves the Laplacian operator (\\Delta) acting on the velocity vector (\\vec{v}), and multiplying by the kinematic viscosity (\\nu). It accounts for the internal friction within the fluid, causing velocity gradients and damping the flow.\n\n- $\\vec{g}$: This term represents the gravitational force acting on the fluid. It corresponds to the vector gravitational acceleration (\\vec{g}) and influences the fluid flow by causing it to move in the direction opposite to the gravitational field.\n\nOverall, this equation, known as the Navier-Stokes equation, describes the conservation of momentum in a fluid and captures the various forces and terms influencing the fluid flow.\n","lastmodified":"2023-10-07T08:19:48.786592134Z","tags":[]},"/notes/python/Object-Oriented-Programming":{"title":"Object Oriented Programming","content":"","lastmodified":"2023-10-07T08:19:48.786592134Z","tags":[]},"/tags/setup":{"title":"setup","content":"","lastmodified":"2023-10-07T08:19:48.786592134Z","tags":[]}}